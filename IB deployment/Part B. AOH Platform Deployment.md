__Table of Content__

&nbsp;&nbsp;&nbsp;&nbsp;[1 AOH Platform Architecture](#1-aoh-platform-architecture)

&nbsp;&nbsp;&nbsp;&nbsp;[2 Deployment Preparation](#2-deployment-preparation)

&nbsp;&nbsp;&nbsp;&nbsp;[3 Deployment](#3-deployment)

&nbsp;&nbsp;&nbsp;&nbsp;[4 Troubleshooting Tricks](#4-troubleshooting-tricks)

# 1 AOH Platform Architecture
A high level AOH platform diagram is attached below:

![AOH Platform Architecture](./aoh-platform-architecture.drawio.svg)
 
- AOH platform is based upon Kubernetes cluster hosting environment. It adopts micro-service architecture.
- This document explains a deployment hosting in AWS cloud environment. It makes use of services like EKS, Secret manager and ELB. The AWS secret manager contains a secret containing multiple key-value pairs. It links to a kubernetes ClusterSecretStore that AOH platform components refer to. Some key-value pairs are administrator password for the common services in the platform, such as the root username, password of MinIO. Those common services will boot up with this credential settings. The other key-value pairs are credential of service accounts. The service accounts are created for service to service authentication, for example, a service account is created in KeyCloak for AOH web module to allow it access other protected services in AOH.
- Traefik is the reverse proxy engine. It routes and exposes services URL endpoint using a registered public domai. It also links the public domain certificate to support HTTPS.
- ArgoCD is the continuous deployment tool. It is not drawn in the diagram. It is installed to the Kubernetes cluster and deploys all AOH platform components while actively monitors any changes in the target GIT repositories.
- Keycloak is used as AOH platform access managment Gateway. It serves both for service to service authentication, and for platform users access managment. It can integrate with customer's own identity provider (such as Azure AD) to implement SSO.
- Common services are shown in the diagram: MinIO the object store, NATS the messaging broker, PostgreSql the relationship database and Hasura the graphQL API layer on top of Postgresql database. 

# 2 Deployment Preparation
## 2.1 Install ArgoCD
ArgoCD is the continuous deployment tool for AOH platform. After installing ArgoCD to the Kubernetes cluster, it will actively monitor the specified GIT repositories and keeps cluster Kubernetes cluster components and configuration matching with the deployment script.

`NOTE: The starting CD script is ./ar2-infra/argocd/new-cluster-name/root-app.yml. We will explain in a later step how to config ArgoCD using the deployment scripts.`

```bash
# Create namespace before installing ArgoCD
kubectl create namespace argocd
 
# Install ArgoCD
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

After the installation, ArgoCD service will startup with an autogenerated password for admin user. Fetch the password and note it down so we are able to access ArgoCD web portal, which will be the main user interface to assist AOH deployment and troubleshooting. 
```bash
# Get ArgoCD access credentials for admin
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
```

To access the ArgoCD web portal, you may use kubectl to port-forward the argocd service port to a localhost port.
```bash
# Forward UI for access, for example, forward to localhost 19080 port
kubectl port-forward --address localhost -n argocd svc/argocd-server 19080:80
```

After port-forwarding, you should be able to access ArgoCD web portal login page by typing url "http://localhost:19080" in a web browser. Login with "admin" as username and the previously fetched credential.

![ArgoCD Login Page](./imgs/argocdLoginPage.png)

Config ArgoCD to connect required GIT repositories (Those repositories are created in Part A document).

![ArgoCD Connect Repositories](./imgs/argocdConnectRepositories.png)


## 2.2 Prepare Secrets
In a deployment using AWS cloud, AOH platform relies on AWS Secret manager service to manage platform secrets. 
There are two major types of secrets:
- Part 1:  Initialize admin credential for several common components (MinIO Etc.), 
- Part 2: Service accounts to be created within KeyCloak or MinIO so other micro-services can access KeyCloak and MinIO.

It is highly recommended to proerly config Part 1 secrets in the prepartion phase. All secrets can be filled into a JSON text file and imported into AWS secret manager later.

The explaination of the complete list of secrets is as below:

- Part 1 (Common Component Initial Credential)

    | Secret | Remark | Sample |
    |----------|---------:|----------|
    | postgresql    |      | "postgresql": {"user": "postgres","password": "admin password"}     |
    | replay-postgresql   |      | "replay-postgresql": {"user": "postgres","password": "postgres"}     |
    | keycloak    |      | "keycloak": {  "admin_password": "admin password" }     |
    | minio    |      |   "minio": { "root_user": "minioadmin", "root_password": "admin password" }  |
    | hasura    |      |  "hasura": { "url": "https://hasura.your-own-domain/v1/graphql", "admin_secret": "admin password", "database_url": "postgres://postgres:postgres@ar2-postgres-dbz-postgresql.ar2:5432/ar2"  }   |
    | replay-hasura    |     | "replay-hasura": {"admin_secret": "admin password", "database_url": "postgres://postgres:postgres@replay-postgres.common-rnr.svc.cluster.local:5432/ar2"  }  |
    | hasuracookie    |      | "hasuracookie": {"admin_secret": "admin password", "database_url": "postgres://postgres:postgres@ar2-postgres-dbz-postgresql.ar2:5432/ar2", "metadata_database_url": "postgres://postgres:postgres@ar2-postgres-dbz-postgresql.ar2:5432/ar2_reader"  }    |
    | replay-hasuracookie    |      | "replay-hasuracookie": {"admin_secret": "admin password", "database_url": "postgres://postgres:postgres@replay-postgres.common-rnr.svc.cluster.local:5432/ar2",    "metadata_database_url": "postgres://postgres:postgres@replay-postgres.common-rnr.svc.cluster.local:5432/ar2_reader" }    |

- Part 2 (Service Accounts Credential to be created in KeyCloak and MinIO)
    
    Part 2 service accounts can only be created after initial deployment is done and common services such as MinIO and KeyCloak spin up. In the preparation stage, filling a placeholder string such as <youkey> in the JSON secret temporarily.  
    | Secret | Remark | Sample |
    |----------|---------:|----------|
    | common-iam    |  Create service account in MinIO for KeyCloak to access    | "common-iam": { "store": { "access_key": "..", "secret_key": ".." }  }    |
    | common-rnr    |   Create service account in KeyCloak for rnr module; create service account in MinIO for rnr module   | "common-rnr": {  "iam": { "client_secret": ".." }, "store": {  "access_key": "..","secret_key": ".."  } }   |
    | common-wf    |  Create service account in KeyCloak for eSOP module    | "common-wf": {  "iam": {  "client_secret": ".."   }}   |
    | demo-tms    |   Create service account in KeyCloak for demo-tms module    |   "demo-tms": { "iam": {  "client_secret": ".."    }  } |
    | common-ums   |   Create service account in KeyCloak for ums module; Create service account in MinIO for ums module    | "common-ums": {  "iam": { "client_secret": ".."}, "store": { "access_key": "..",  "secret_key": ".." }}   |
    | common-ucs    |  Create service account in MinIO for ucs module  | "common-ucs": {  "store": {  "access_key": "..",  "secret_key": ".."   }}   |
    | ar2-import    |  Create service account in MinIO for ar2-import module    | "ar2-import": { "store": { "access_key": "..", "secret_key": ".."  }}   |
    | ar2-ucs    |   Create service account in MinIO for ar2-ucs module; Let ar2-ucs know the postgresql database access credential   |  "ar2-ucs": {  "store": {  "access_key": "..", "secret_key": ".."  }, "postgresql": { "user": "postgres", "password": "postgres" }}  |
    | ar2-web    |    Create service account in MinIO for ar2-web module; Create service account in KeyCloak for ar2-web module; Let ar2-web module know MapBox access credential  |  "ar2-web": { "store": { "access_key": "..", "secret_key": ".." }, "iam": { "client_secret": ".." }, "map": {"mapbox_key": "..",      "mapbox_secret_access_token": "..", "public_mapbox_access_token": ".."  }}  |

- Part 3 (Others)
    | Secret | Remark | Sample |
    |----------|---------:|----------|
    | github    | Github Personal access tokens to access the repositories     | "github": { "cred": "{\"auths\":{\"ghcr.io\":{\"username\":\"user1\",\"password\":\"your personal access token\",\"auth\":\"ask AOH team for it\"}}}" |

You can import the JSON file to AWS secret manager using AWS console webpage or CLI. The CLI command is 
```bash
 aws secretsmanager update-secret --secret-id aohsandbox-secret --secret-string file://yoursecretprofile.json
 ```

## 2.3 Install Traefik
```bash
# Deploy traefik (from init folder)
kubectl create ns traefik
helm repo add traefik https://traefik.github.io/charts
 
cd ar2-infra\argocd\new-cluster-name\init\traefik\
helm install traefik traefik/traefik -f ./values-for-aws-wfm.yml -–namespace traefik
```
NOTE: There is an error in value-for-aws-wfm.yml file that the correct value for "enabled" key is **true** but not double quotation enclosed **"true"**
```yaml
        ...
        service:
        enabled: true
        type: LoadBalancer
        ...
```

## 2.4 Create AOH stroage class
```bash
cd ar2-infra\argocd\new-cluster-name\init\k8s\
kubectl apply -f sc—retain.yml
```

## 2.5 Domain preparation
Follow below steps to publish the AOH platform to public domain.
- Use AWS Route 53 service to create a public domain for the AOH deployment. Register a public domain

![AWS register Route 53 domain](./imgs/AWSRegisterDomain.png)

- User AWS Certificate Manager service to create a certificate for the public domain. Note use wildcast domain name such as *.stenmss.org. Take note its ARN number by clicking into the certificate

![AWS create domain certificate](./imgs/AWSCertificateManager.png)

- Update this cert ARN to file ar2-infra\argocd\new-cluster-name\init\traefik\values-for-aws-wfm.yml
    ```yml
    ...
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "ssl"
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert:  arn:aws:acm:ap-southeast-1:654654173638:certificate/4955e9ec-9056-428e-8cbf-6ded4b2053d4
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
    ..
    ```
    And update traefik service setting
    ```bash
    helm upgrade traefik traefik/traefik -f values-for-aws-wfm.yml -n traefik
    ```
    Note down the loadbalander name the traefik service is attached to 

    ![AWS traefik loadbalancer](./imgs/AWSTraefikLoadBalancer.png)

- Create records in AWS route 53 hosted zone of the public domain. These records will allow you to access ArgoCD, MinIO etc in browser without the need for port-forwarding any more. A sample record creation is as below. Choose the traefik load balancer to create record.

    ![AWS Sample domain record creation](./imgs/AWSSampleDomainHostRecord.png)

    The screenshot below shows a few records being created in the hosted zone.

    ![AWS Sample domain records](./imgs/samplehostedzonerecords.png)

    A few important url records are 
    - 
    - iam-admin.stenmss.org/admin/master/console/#/ar2/clients (KeyCloak portal)
    - cd-admin.stenmss.org (ArgoCD portal)
    - hasura.stenmss.org (Hasura portal)
    - store-admin.stenmss.org (MinIO portal)
    - web.stenmss.org (AOH portal)
    - web.stenmss.org/workflow-designer (ESOP designer view)
- Modify the Traefik ingress route setting to the correct domain name. 

    There are a lot of ingressroute setting currently not pointing to your public doamin in ar2-infra and ar2-web-infra. Such as in ar2-infra\argocd\new-cluster-name\manifests\traefik folder, a file admin-argo.yml uses cd-admin.wfm-qa.agilrad.com. You need to search in all repositories for all occurance of "wfm-qa.agilrad.com", replace them with your own public domain name, such as "stenmss.org"
    


## 2.6 Fix mistakes in original AOH Deployment Document
- The installed Traefik has different version with the original document. replace **traefik.containo.us/v1alpha1** with **traefik.io/v1alpha1** in ar2-infra and ar2-web-infra repositories.
    ```
    NOTE: the ingressroute "match" grammar has been changed. you may encounter wrong defnition such as "PathPrefix(`/realms/ar2/`, `/resources/`)". The correct setting should be "(PathPrefix(`/realms/ar2/`) || PathPrefix(`/resources/`))"
    ```
    Check files in ar2-infra\argocd\new-cluster-name\manifests\traefik to fix. There are mistakes in admin-traefik.yml, ops-iam.yml
- The original document does not mention, but you need to use kubectl to import all the yaml files under ar2-infra\argocd\new-cluster-name\init\minio\
- The original document does not mention, but you need to use kubectl to import all the yaml files under ar2-infra\argocd\new-cluster-name\init\external-secrets\secrets\
- The original document does not mention, but you need to use kubectl to import ar2-infra\argocd\new-cluster-name\init\k8s\priorityClass.yml


## 2.8 Suggestions
- Limit cronjob spawning a lot of cron job PODS when Kubernetes cluster is scaled down

    In a sandbox environment, you may consider scaling down Kubernetes cluster to zero worker nodes to save cost. You must stop the below two cronjobs spawning too many PODS when the kubernetes cluster is scaled down, otherwise when you scale the cluster back to normal, there will be overwhelming pending job PODS that will take tremendor time to resolve and slow down the resume of cluster.
    The two cronjobs you need to modify are ar2-infra\argocd\new-cluster-name\manifests\ar2-rnr\periodic-backup-raw-cronjob.yml and ar2-infra\argocd\new-cluster-name\manifests\ar2-rnr\periodic-cleanup-cronjob.yml. Modification sample is as below
    ```yml
    schedule: "*/15 * * * *"
    concurrencyPolicy: Replace  # Replace with new job if a previous one is pending
    startingDeadlineSeconds: 300  # Jobs must start within 5 minute of their scheduled time
    failedJobsHistoryLimit: 1
    successfulJobsHistoryLimit: 1
    ```

# 3 Deployment
It is essential to kick off the continuous ArgoCD deployment loop first, even many AOH platform component won't be able to boot up correctly the first time because of wrong or missing settings. The main purpose is to start up common services, such as KeyCloak, MinIO etc so we are able to config the platform correctly step by step. The procedure is not a one step deployment procedure instead it is a continuous, progressive procedure. Make use of ArgoCD portal and Kubectl in the procedure to troubleshoot and complete the deployment eventually.

## 3.1 Spin up ArgoCD Deployment
```bash
cd ar2-infra/argocd/new-cluster-name/
# Add root and project manifests (triggers sync)
kubectl apply -f root-app.yml
kubectl apply -f projects/project.yml
```
root-app.yml file is the entry yaml that ArgoCD will start the automatic and continuous deployment of AOH platform.
Check you can access the below portals (change the domain name accordingly to yours):
- https://store-admin.stenmss.org/

This step should also expose ArgoCD service portal with your public domain name, try if you can access ArgoCD without port forwarding:
-  https://cd-admin.stenmss.org/

Troubleshoot and make sure the above common services portal working before continue with the following steps.

## 3.2 Upload Jar file in MinIO
- Get jar file stengg.agilrad.keycloak.user-user-storage-provider-v1.4.0.jar from AOH team. It shall be uploaded to MinIO bucket common-iam.
The file is related with KeyCloak user federation creation.

- In MinIO portal, create a service account and config AWS secret manager secret key "common-iam" with the access key and secret key. This is too allow KeyCloak service being able to access MinIO to download the Jar file. 

After this step, monitor the common-keycloak-20 application in ArgoCD. Wait and troubleshoot until it move to health status. You should be able to access below KeyCload portal after KeyCloak service starts up successfully. Note the secret propagation in Kubernets cluster takes time. Use ArgoCD to check application log to troubleshoot.
- https://iam-admin.stenmss.org/admin

## 3.3 Create ar2 realm in Keycloak
In KeyCloak portal, create an "ar2" realm. You can get the realm creation json file from AOH team.

![Create Realm in KeyCloak](./imgs/realmCreation.png)

## 3.4 Create service accounts in MinIO and KeyCloak
- Create two service accounts in MinIO, add the access key and secret key to aws secret manager secret under key "common-ucs" and "ar2-web"
- Use keycloak to regenerate client secret for these clients: aoh_web, aoh_ums, aoh_rnr,aoh_wfe,demo_trip and fill into aws secret manager secret for key: ar2-web, common-ums, common-rnr, common-wf, demo-tms

## 3.5 check redirect URI in Keycloak for clients
Check redirect URIs of client aoh_web in keycloak. Make sure it correctly points to the correct domain.

## 3.6 Database dumping 
`Note: The script can only run correctly in Linux environment`

Locate aoh_db-0.0.4 folder in aoh-db repository and load the database content in.
- run **11-clear-db.sh**
- run **12-deploy-from-migration.sh**

## 3.7 UCS Video call setting
- Find the public IP address of the sfu load balancer. The load balancer name is like k8s-commonuc-sfu-... Find the load balancer in AWS console. Use nslookup with its DNS name to find out its public IP address.
- Fill the public IP addres to ar2-infra\argocd\new-cluster-name\manifests\ar2-ucs\sfu-config.yaml, under the nat1to1 setting.

`NOTE: The web socket connection does not stay alive very long in OA network`

# 4 Troubleshooting Tricks
- Use ArgoCD to check problemtic POD logs
- Use Kubectl to check the services status and POD environment variables (sometimes the external secret propagation takes long time, force argoCD redeployment may help)

# 5 Defects in current AOH Deployment
- The record and replay module does not work in my deployment yet. 

  One manual step I noted down is to create jetstream in nats server
    ```bash
    # create a temporary linux and pod and install nats CLI in
    kubectl run nats-cli-pod --image=alpine --restart=Never -it -- /bin/sh
    # inside the POD
    apk update
    apk add curl bash ca-certificates
    curl -sf https://binaries.nats.dev/nats-io/natscli/nats@latest | sh

    ./nats context save debeziumCtx --server=nats-aohsandbox.common-msg:4222 --user={username} --password={password}

    // set the current context
    ./nats context select debeziumCtx

    // create new DebeziumStream
    ./nats stream add --description="The debezium stream, contains messages which are comming from debezium" --subjects=aoh.aoh_sys.postgres.*.* --replicas=1 --storage=file --retention=limits --ack --discard=old --dupe-window=2m0s --no-deny-delete --no-deny-purge --no-allow-rollup --max-msgs=-1 --max-msgs-per-subject=-1 --max-bytes=8000000000 --max-age=7d --max-msg-size=-1 --max-consumers=-1 DebeziumStream
    ```

- The database configuration needs a lot of manual efforts. There are various issues that can only be manully reolved by developer through creating schema/role/record, granting schema privilidge to role, fixing "dirty database" errors etc. AOH team please review the DB creating procedure.



# 6 Expected Enhancements
- Set priority class properly to start important services earlier, such as keycloak/hasura/postgresql/MinIo. It will shortent cluster booting speed.

- Automatet the procedure to create a new deployment with customized domain name. The current procedure still relies on text string batch replacement in folders.  

- The video call setting relies on the sfu load balancer ip address. There should be a method to fix a dedicated load balancer for SFU with elastic IP address.